---
title: Linear Model
date: 2025-08-21
---

## Linear Classification
We want to classification some data points $x$ by a linear model $h$.
### Model
We use a hypothesis set of linear classifiers, where each h has the form
$$
h(x)\ =\ sign(w^T x)
$$
for some $w\ \in R^{d+1}$, where $d$ is the dimensionality of the input space.
Note that we add coordinate $x_0\ =\ 1$ corresponds to the bias 'weight' $w_0$

### Feasibility
1. Generalization Bound
	With high probability,
>[!green] VC generalization bound 
>$$
>E_{out}(g)\ =\ E_{in}(g)\, +\, \mathcal{O} \left( \sqrt{\frac{d}{N}\ln N}\ \right)
>$$

2. $E_{in}$ can be small
	If there is a linear hypothesis that has small $E_{in}$ , then PLA algorithm can help us find it.

### Algorithm
Because of noise, rather to solve $E_{in}\ =\ 0$, we would like to find a hypothesis with the minimum $E_{in}$ , we need to solve the combinatorial optimization problem:
$$
\min_{w\, \in\, \mathbb{R^{d+1}}} \frac{1}{N}\ \sum_{n\, =\, 1}^{N} \left[\!\left[ sign(w^Tx_n)\ \neq\ y_n \right]\!\right] 
$$
> [!abstract] **The Pocket Algorithm**
> <div style="text-align:center;">
> <img src="https://i.postimg.cc/0y0jByCr/2025-08-21-8-10-32.png" alt="Example Image">
> </div>



**Remark**
> [!abstract] **Perceptron Learning Algorithm**
> 
> <div style="text-align:center;">
> <img src="https://i.postimg.cc/x11V9j2X/2025-08-21-8-13-45.png" alt="Example Image">
> </div>

## Linear Regression
### Model
$$
h(x)\ =\ \sum_{i\,=\,0}^d w_ix_i\ =\ w^Tx
$$
where $x_0\ =\ 1$ and $x\ \in\ {1} \times\ \mathbb{R}^{d}$, $w\ \in\ \mathbb{R}^{d+1}$

### Feasibility
>[!green] VC generalization bound 
>$$
>E_{out}(g)\ =\ E_{in}(g)\, +\, \mathcal{O} \left( \frac{d}{N} \right)
> $$


### Algorithm
#### Analysis
$$
\begin{aligned}
	&E_{out}(h)\ =\ \mathbb{E} \left[ (h(\mathbf{x})\ -\ y)^2\right] \\
	\\
	&E_{in}\ =\ \frac{1}{N} \sum_{n\,=\,1}^{N} \left( h(\mathbf{x_n}\ -\ y_n)^2 \right)
\end{aligned}
$$

Define the data matrix $X \in\ \mathbb{R}^{N\,\times\,(d+1)}$  whose rows are the inputs $x_n$ as row vectors.
Define the target vector $y\ \in\ \mathbb{R}^N$.
$$
E_{in}\ =\ \frac{1}{N}\left( w^TX^TXw\,-\,2w^TX^Ty\,+\,y^Ty \right)
$$
We want to solve the optimization problem:
$$
w_{lin}\ =\ \underset{w\,\in\,\mathbb{R}^{d+1}}{argmin}\,E_{in}(w)
$$
To get $\nabla E_{in}(w)\ =\ 0$, one should solve for $w$ satisfies
$$
X^TXw\ =\ X^Ty
$$
If $X^TX$ is invertible,
>[!gray] Solution for invertible matrix
>$$
>w\ =\ X^†y,\ \text{where}\ X^†\ =\ (X^TX)^{-1}X^T\ \text{is the}\ pseudo-inverse\ \text{of X}
>$$

Otherwise, if $X^TX$ is not invertible,
Let $\rho$ be the rank of $X$.
Assume that the SVD of $X$ is $X\ =\ U\Gamma V^T$, where
$$
\begin{aligned}
U\ \in\ \mathbb{R}^{N \times \rho}\quad &\text{satisfies}\quad UU^T\ =\ I_{\rho} \\
V\ \in\ \mathbb{R}^{(d+1)\,\times\,\rho}\quad &\text{satisfies}\quad V^TV\ =\ I_{\rho} \\
\Gamma\ \in\ \mathbb{R}^{\rho \times \rho}\quad &\text{is a positive diagonal matrix}
\end{aligned}
$$
Then,
> [!gray] Solution for non-invertible matrix
> $$
> w_{lin}\ =\ V\,\Gamma^{-1}U^T\mathbf{y}\quad\quad \text{is a solution}
> $$

#### Pseudo Code
<div style="text-align:center;">
<img src="https://i.postimg.cc/G3FWYT3B/2025-08-21-8-15-17.png" alt="Example Image">
</div>

#### Others
The  $\textbf{\textcolor{pink}{hat matrix}}$
$$
H\ =\ X\left( X^TX\right)^{-1}X^T
$$
has the properties that $H^2\ =\ H$, which can facilitate the analysis of error.


## Logistic Regression
### Description
Given a data set $\mathcal{X}$  with data points $(x,\,y)$ , where $x$ is a vector and $y\ \in\ \{-1,\,1\}$. The logistic regression model is designed to estimate the probability $\mathbb{P}[y\ =\ 1\,|\,x]$ for a given input $x$.

### Model
$$
h(\mathbf{x})\ =\ \theta(w^Tx)
$$
where $\theta$ is the so-called $\textbf{\textcolor{pink}{logistic}}$ function $\theta(s)\ =\ \frac{e^s}{1\,+\,e^s}$ .

Another popular soft threshold is the $\textbf{\textcolor{pink}{hyperbolic tangent}}$
$$
\tanh{s}\ =\ \frac{e^s\,-\,e^{-s}}{e^s\,+\,e^{-s}}
$$

### Algorithm
#### Error measure
We are trying to learn the target function
$$
f(\mathbf{x})\ =\ \mathbb{P}[y\ =\ +1\,|\,\mathbf{x}]
$$
However, the given data set is generated by a noisy target function
$$
P(y\,|\,x)\ =\ 
\begin{cases}
f(x)\quad,\ &if\ y\ =\ +1;\\ \\
1\ -\ f(x), &if\ y\ =\ -1.
\end{cases}
$$
Hence, the target distribution captured by our hypothesis $h(x)$ is
$$
P(y\,|\,x)\ =\ 
\begin{cases}
h(x)\quad,\ &if\ y\ =\ +1;\\ \\
1\ -\ h(x), &if\ y\ =\ -1.
\end{cases}
$$
We would like to maximum the product
$$
\prod_{n\,=\,1}^{N} P(y_n\,|\,x_n)
$$
The method of $\textbf{\textcolor{pink}{maximum likelihood}}$ selects the hypothesis $h$ which maximizes this probability. Below, there are two equivalent error measurement.

1.  For the reason of simplicity, we solve an equivalent problem: 
$$
E_{in}(w)\ =\ \frac{1}{N}\sum_{n\,=\,1}^{N}\ln{\left( \frac{1}{P(y_n\,|\,x_n)}\right)}\ =\ \frac{1}{N}\sum_{n\,=\,1}^{N} \ln{\left(1\ +\ e^{-y_nw^Tx_n}\right)}
$$
2. $\textbf{\textcolor{pink}{Cross-entropy error measure}}$	
	The maximum likelihood method reduces to the task of finding $h$ that minimizes
$$
E_{in}(\mathbf{w})\ =\ \sum_{n\,=\,1}^{N}\left[\!\left[ y_n\ =\ +1\right]\!\right]\ln{\frac{1}{h(x_n)}}\ +\ \left[\!\left[ y_n\ =\ -1\right]\!\right]\ln{\frac{1}{1\ -\ h(x_n)}}
$$

>[!blue] Discussion
> $$
> \nabla E_{in}(w)\ =\ -\frac{1}{N} \sum_{n\,=\,1}^{N} \frac{y_nx_n}{1\ +\ e^{y_xw^Tx_n}}
> $$
> A misclassified example contributes more to the gradient than a correctly classified one.


#### Gradient descent
1. $\textbf{\textcolor{pink}{Batch gradient descent}}$
<div style="text-align:center;">
<img src="https://i.postimg.cc/gjg4sWB7/2025-08-21-8-16-58.png" alt="Example Image">
</div>

Note that the $\textbf{\textcolor{pink}{learning rate}}$ have to be specified. $\textcolor{lime}{\text{A good choice for }\eta\text{ is around 0.1}\,.}$

2. $\textbf{\textcolor{pink}{Stochastic gradient descent (SGD)}}$
<div style="text-align:center;">
<img src="https://i.postimg.cc/v80vpG3q/2025-08-21-8-17-42.png" alt="Example Image">
</div>

- $\textbf{Explanation}$
	Consider the expected weight change, we may find that this is exactly the same as the deterministic weight.
- $\textbf{Termination}$
	A good stopping criterion should consider the total error on all the data.
#### Pseudo Code
<div style="text-align:center;">
<img src="https://i.postimg.cc/pT2D6S8M/2025-08-21-8-18-12.png" alt="Example Image" style="height: 220px;">
</div>

- $\textbf{Initialization}$
<span class = 'lime' style="display: block; margin-left: 20px;">Choosing each weight independently from a Normal distribution with zero mean and small variance usually works well in practice.</span>

- $\textbf{Termination}$
<span class = 'lime' style="display: block; margin-left: 20px;">A maximum number of iterations, marginal error improvement, coupled with small value for the error itself works reasonably well.</span>
	
## Nonlinear Transformation

We can view the hypothesis as a linear one after applying a nonlinear transformation on $\mathbf{x}$ .
The transform $\Phi$ that takes us from $\mathcal{X}$ to $\mathcal{Z}$ ($\text{\textbf{\textcolor{pink}{feature space}}})$ is called a $\text{\textbf{\textcolor{pink}{feature transform}}}$ .

For example, the feature transform $\Phi_Q$ for degree-Q curves in $\mathcal{X}$ is called the $\text{\textcolor{pink}{Q-th}}$ 
$\text{\textbf{\textcolor{pink}{order polynomial transform}}}$ .

> [!green] VC dimension
> If the transform $\Phi_Q$ maps a two-dimensional vector $\mathbf{x}$ to $\tilde{d}\ =\ \frac{Q(Q\,+\,3)}{2}$ dimensions in $\mathcal{Z}$. 
> $$
> d_{VC}\ =\ \frac{Q(Q\,+\,3)}{2}\ +\ 1
> $$

> [!blue] Discussion
> Consider the following feature transform, which maps a $d$-dimensional $\mathbf{x}$ to a one-dimensional $\mathbf{z}$, keeping only the k-th coordinate of $\mathbf{x}$ .
> $$
> \Phi_k(x)\ =\ (1,\,x_k)
> $$
> The hypothesis set $\mathcal{H}$ is called the $\textbf{\textcolor{pink}{Decision stump model}}$ on dimensional k.











